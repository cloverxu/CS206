{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ensembles\" are an approach for making predictions that involves combining several models together. \n",
    "\n",
    "The concept is illustrated below with a logistic regression model, a decision tree and a k nearest neighbours model. Instead of making a prediction with one of the models, predictions are made by taking input from all of the models. Specifically, given some features X, a combined prediction y  is found by first obtaining predictions from each of the models and combining it by some method: for example voting (taking the majority of class predictions) or taking the average (also applicable in regression). \n",
    "\n",
    "![title](images/ensembles.png)\n",
    "\n",
    "The approach has significant benefits and many of the most effective algorithms in data mining are based on ensembles (for example Random Forests, Ada Boost). Benefits of ensembles includes that predictions from several models combined on average has a lower error than any of the individual models separately, the combined model also generally has less tendency for overfitting. \n",
    "\n",
    "This can be understood to be because the variance (i.e. of the predictions) is reduced and if the models used in constructing the combined model are independent (that is they are have different data and/or different modelling processes used in determining them) then the resulting model will have less \"bias\" than any of the indivudal models (e.g. linear models have a particular bias to finding linear patterns and combining linear and non linear models prevents this bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, boosting and stacking are three variants or approaches for generating ensembles. They should be thought of as \"Meta\" algorithms ( algorithms that have as input or operate on several sub algorithms that themselves act on the data).\n",
    "\n",
    "#### Bagging\n",
    "Creating several models (which are then combined) by generating additional training data through random resampling from the original data set and then using different subsets of this data to different train models. See also http://scikit-learn.org/stable/modules/ensemble.html#bagging\n",
    "\n",
    "#### Boosting\n",
    "Boosting is a more complex sequential approach (selection of models and data depends on the previous step). It extends the training data selection idea used in bagging by selectively choosing data to train the models in the ensemble in order to create models to better classify training examples that are misclassified by other models already in the ensemble. \n",
    "\n",
    "In boosting an initial set of models is generated from subsets of the training data, then additional models are trained by selecting training data that consists of examples that were most likely to be misclassified by the original models obtained in the first step. See also http://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "#### Stacking\n",
    "In stacking it is generally understood that several model learning algorithms are used rather than just varying the data. In addition, instead of using simple mathematical function such as the mean, or majority or weighted voting to combine predictions, a data mining model (such as logistic regression) can be used to combine the outputs/predictions of many indivudal models together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iris Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example of classifying with several models on the Iris data set in which we explicitly create our models with different algorithms and then combine them. We use only 2 feature columns which makes the problem more difficult. You will learn the Naive Bayes algorithm in the next Lecture. Random Forests are in themselves ensemble classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate invidual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some models with cross validation and look at the accuracy. Try to run the cell several times to observe the different scores - generally around 90 - 95 percent. Also see confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.95 (+/- 0.03) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3], ['Logistic Regression', 'Random Forest', 'naive Bayes']):\n",
    "    scores = model_selection.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement ensemble classifier with voting. \n",
    "\n",
    "Two approaches to voting are majority voting in which the ensemble prediction is majority class predicted by the component models and weighted voting in which probability/confidence levels are used to adjust the vote (may be based on class probabilities produced by the algorithm and/or derived from an estimate of the model performance). \n",
    "\n",
    "For example if we had predictions clf1 => Versicolor, clf2 => Versicolor, clf3  => Setosa by majority voting the ensemble would predict Versicolor. \n",
    "\n",
    "We use an approach of weighted average probabilities that combines a weight for the model and the class probabilities to produce a value between 0 and 1 that is the combined prediction for each class weighted by the models weight, ie for each possible class calculate: weight_model1 \\* probability_model1 + weight_model2 \\* probability_model2 + ... \n",
    "\n",
    "We take the highest value. For example if the probabilities of class membership from the three models are as follows:\n",
    "- clf1: Setosa 0.2, Versicolor 0.5, Virginica 0.3\n",
    "- clf2: Setosa 0.6, Versicolor 0.3, Virginica 0.1\n",
    "- clf3: Setosa 0.3, Versicolor 0.4, Virginica 0.3\n",
    "\n",
    "The highest score is for versicolor as (1\\*0.5 + 1\\*0.3 + 1\\*0.4)/3 = 0.4\n",
    "\n",
    "Note: the scores for Setosa and Verginica were:\n",
    "- Setosa (1\\*0.2 + 1\\*0.6 + 1\\*0.3)/3 = 0.37\n",
    "- Virginica (1\\*0.3 + 1\\*0.1 + 1\\*0.3)/3 = 0.23\n",
    "\n",
    "In the following implementation we use the concept of classes in Python (see https://docs.python.org/3/tutorial/classes.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    \"\"\"\n",
    "    Paramters for constructor\n",
    "    clfs: iterable list of scikit-learn classifier objects\n",
    "    weights: list of weights for the clf (float or int is acceptable)\n",
    "    \"\"\"\n",
    "    def __init__(self, clfs, weights=None):\n",
    "        self.clfs = clfs\n",
    "        self.weights = weights    \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Fit some data to each of the models in the ensemble    \n",
    "    X : numpy array          \n",
    "    y : list or numpy array \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.clfs:\n",
    "            clf.fit(X, y)\n",
    "\n",
    "    \"\"\"\n",
    "    Generate predictions with majority voting   \n",
    "    X : numpy array  \n",
    "    Returns : numpy array containing predicted class labels by ensemble \n",
    "    \n",
    "    \"\"\"\n",
    "    def predict(self, X):            \n",
    "        self.classes_ = np.asarray([clf.predict(X) for clf in self.clfs])\n",
    "        if self.weights:\n",
    "            avg = self.predict_proba(X)\n",
    "            # keep majority vote \n",
    "            maj = np.apply_along_axis(lambda x: max(enumerate(x), key=operator.itemgetter(1))[0], axis=1, arr=avg)\n",
    "        else:\n",
    "            maj = np.asarray([np.argmax(np.bincount(self.classes_[:,c])) for c in range(self.classes_.shape[1])])\n",
    "        \n",
    "        return maj\n",
    "\n",
    "    \"\"\"\n",
    "    Generate predictions with probability majority voting   \n",
    "    X : numpy array  \n",
    "    Returns : numpy array containing  Weighted average probability for each class per sample \n",
    "    \n",
    "    \"\"\" \n",
    "    def predict_proba(self, X):\n",
    "        self.probas_ = [clf.predict_proba(X) for clf in self.clfs]\n",
    "        avg = np.average(self.probas_, axis=0, weights=self.weights)\n",
    "\n",
    "        return avg        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the weighted voting schema we just implemented and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.03) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# set equal weights for each of the component classifiers\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "\n",
    "# perform the test as before\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = model_selection.cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell several times. Which of the classifiers performs the best each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the weights (and other paramters) by iterating through all possibilities if the size of the problem is not too large..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x140840a8b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.045216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.044222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.032660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.038873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.044222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.044222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.047140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.044222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.038873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.038873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.045216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.048990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.044222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.048990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     w1   w2   w3      mean       std\n",
       "0   1.0  1.0  1.0  0.946667  0.049889\n",
       "1   1.0  1.0  2.0  0.946667  0.045216\n",
       "2   1.0  1.0  3.0  0.940000  0.044222\n",
       "3   1.0  2.0  1.0  0.940000  0.032660\n",
       "4   1.0  2.0  2.0  0.940000  0.038873\n",
       "5   1.0  2.0  3.0  0.926667  0.044222\n",
       "6   1.0  3.0  1.0  0.953333  0.040000\n",
       "7   1.0  3.0  2.0  0.946667  0.033993\n",
       "8   1.0  3.0  3.0  0.926667  0.044222\n",
       "9   2.0  1.0  1.0  0.953333  0.040000\n",
       "10  2.0  1.0  2.0  0.946667  0.033993\n",
       "11  2.0  1.0  3.0  0.933333  0.047140\n",
       "12  2.0  2.0  1.0  0.940000  0.044222\n",
       "13  2.0  2.0  2.0  0.946667  0.033993\n",
       "14  2.0  2.0  3.0  0.946667  0.033993\n",
       "15  2.0  3.0  1.0  0.940000  0.038873\n",
       "16  2.0  3.0  2.0  0.940000  0.038873\n",
       "17  2.0  3.0  3.0  0.953333  0.033993\n",
       "18  3.0  1.0  1.0  0.946667  0.033993\n",
       "19  3.0  1.0  2.0  0.946667  0.045216\n",
       "20  3.0  1.0  3.0  0.940000  0.048990\n",
       "21  3.0  2.0  1.0  0.953333  0.040000\n",
       "22  3.0  2.0  2.0  0.946667  0.033993\n",
       "23  3.0  2.0  3.0  0.940000  0.044222\n",
       "24  3.0  3.0  1.0  0.940000  0.048990\n",
       "25  3.0  3.0  2.0  0.946667  0.049889\n",
       "26  3.0  3.0  3.0  0.946667  0.033993"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "np.random.seed(123)\n",
    "df = pd.DataFrame(columns=('w1', 'w2', 'w3', 'mean', 'std'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[w1,w2,w3])\n",
    "            scores = model_selection.cross_val_score(\n",
    "                                            estimator=eclf,\n",
    "                                            X=X,\n",
    "                                            y=y,\n",
    "                                            cv=5,\n",
    "                                            scoring='accuracy',\n",
    "                                            n_jobs=1)\n",
    "\n",
    "            df.loc[i] = [w1, w2, w3, scores.mean(), scores.std()]\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df['mean'])\n",
    "plt.show() \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the setting for weights important?\n",
    "\n",
    "Adjust the above code to sort show confidence levels in the plot and sort the output list by the performance metric (mean and standard deviation). \n",
    "\n",
    "Hint: the confidence interval formula at 95% confidence level is  u +/- 1.96 * (sd / sqrt(N)? Other confidence levels for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Titanic problem from Practical 4 (see https://github.com/dr-adam/CS206/blob/master/practical4/practical%204.ipynb).\n",
    "\n",
    "Do the following:\n",
    "- Implement an ensemble classifier that uses both logistic regression and decision trees and another 3 algorithms of your choosing.\n",
    "- Tune the weights of that the algorithms in your ensemble contribute to the ensemble classification based on the individual classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is a highly succesful algorithm which has shown good performance on many problems in the literature. Spend some time to read about it here: http://scikit-learn.org/stable/modules/ensemble.html#adaboost and also read some papers describing it here (this can probably safely be read in detail outside the class before or after - just try to skim through and find the key ideas to answer the question below): \n",
    "- https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf (original description)\n",
    "- https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf (multiclass)\n",
    "\n",
    "Write a 1-2 paragraph description of the the way that AdaBoost works and how it incorporates the ideas we just looked at (ensembles, selecting data, bagging and boosting etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we are using the Nursery Data Set which is located in the UCI Machine learning repository (download from here: https://archive.ics.uci.edu/ml/datasets/nursery)\n",
    "\n",
    "Implement AdaBoost to learn models to learn prediction models for the Nursery Data Set (predict the last column which has values: not_recom, recommend, very_recom, priority, spec_prior).\n",
    "\n",
    "Compare your results to those described for the algorithm and data set in this paper: https://ti.arc.nasa.gov/m/profile/oza/files/ozru01a.pdf\n",
    "\n",
    "Note: this question requires you to do quite a lot: \n",
    "- download the data, \n",
    "- read and understand the description of the data in the UCI repository description, \n",
    "- then implement the model, \n",
    "- and finally set up your experiment to use test and training data of different sizes corresponding to the paper Online Bagging and Boosting by Nikunj Oza and Stuart Russel (see section 4 of the above paper with experimental results)\n",
    "- this will enable you to compare your result with the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
